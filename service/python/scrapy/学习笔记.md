

### scrapy笔记
安装 scrapy框架:
#### 安装scrapy 通过` pip install scrapy pypiwin32`
1. 在Windows中安装若遇到
`error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools`

在: http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud 下载python版本对应的扩展包

我的是Python 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)] on win32
则下载 wordcloud‑1.6.0‑cp38‑cp38‑win32.whl
pip install .\wordcloud‑1.6.0‑cp38‑cp38‑win32.whl

提示安装成功即可

2.如果是在ubuntu中,还需要安装第三方扩展库:
`sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev`

#### 创建爬虫

1. 创建项目: `scrapy startproject [爬虫名字]`
2. 创建爬虫: 进入项目所在的路径,执行命令: `scrapy genspider [爬虫名字] [爬虫的域名]`. 注意:爬虫名字和项目名称不能一致.

#### 项目目录结构
1. item.py:用来存放爬虫爬取下来的数据模型.
2. midddlewares.py: 用来存放各种中间件的文件
3. piplines.py:用来将items的模型存储到本地磁盘中
4. settings.py:本爬虫的一些配置信息(比如:请求头,多久发送一次请求,ip代理池等)
5. scrapy.cfg:项目的配置文件
6. spider包:以后所有的爬虫,都可以放在这里面

#### 爬虫笔记
1. response 是一个 ` scrapy.http.response.html.HtmlResponse`对象.可以执行`xpath`和`css`语法来提取数据
2. 提取出来的数据,是一个`selector`或者是一个`SelectorList`对象,如果想要获取其中的字符串,那么应该执行`getall`或者`get`方法.
3. 若使用`xpath`语法则需要使用.extract()方法来获取数据
4. 数据解析回来后, 需要传给pipeline处理,使用`yield`来返回.可以返回需要处理的数据,或者下一个请求的链接.
5. item: 建议在`items.py`中定义好模型.以后不需要再pares()中处理数据.
6. pipeline: 这个是专门用来处理并保存数据的.其中有三个方法会经常使用
  * `open_spider(self,spider)`:当爬虫被打开时执行
  * `process_spider(self,item,spider)`:当爬虫有item传过来时会被调度器调用.
  * `close_spider(self,spider)`: 当爬虫被关闭时,会被调用.
  要激活pipeline,需要在`setting`中找到`ITEM_PIPELINE`一项,并取消注释.
  示例:
  ```python
    ITEM_PIPELINES = {
      # items.py中类名称:管道优先级(0-1000,数值大的先执行)
      'PetsRanking.pipelines.PetsrankingPipeline': 300,
      'PetsRanking.pipelines.PetsrankingPipelinePrint': 400,
    }
  ```

#### JsonItemExport 和 JsonLinesItemExport
保存json数据时,可以使用这两个类
1. `jsonItemExport`:是每次把数据添加到内存中,最后统一写道磁盘,好处是存储的数据是个满足json规则的数据,坏处是如果数据量比较大,那么会很消耗内存
2. `JsonlinesItemExport`:这个是每次调用`export_item`的时候,就把这个item存储到硬盘中.坏处是每个字典占一行,整个文件不是一个标准json格式的文件.好处是每次处理数据的时候就直接存储到硬盘中的.
3.


#### request 和response对象
##### request对象
request对象在我们写爬虫,爬取一页的数据需要重新发送一个请求的时候调用.这个类需要传递一些参数,其中比较常用的参数有:
1. url: 这个request对象发送请求的url.
2. callback: 在下载器下载完相应的数据后执行的回调函数.
3. method: 请求的方法.默认为`get`方法,可以设置为其他方法
4. headers:请求头，对于一些固定的设置，放在settings.py中指定就可以.对于那些非固定的,可以在发送请求的时候指定.
5. meta: 比较常用.用于在不同的请求之间传递数据用的.
6. encoding: 编码.默认为 `utf-8`.使用默认的就可以.
7. dot_filter: 表示不由调度器过滤.在执行多次重复的请求的时候用的比较多
8. errback: 在发生错误的时候执行函数.

##### response对象
response对象一般是由 scrapy 自动构建的. 因此开发者不需要关心如何创建 response 对象, 而是如何使用它. response 对象有很多属性, 可以用来提取数据的.主要有以下属性:
1. meta: 从其他请求传过来的meta 属性, 可以用来保持多个请求之间的数据连接.
2. encoding: 返回当前字符串编码和解码的格式.
3. text: 将返回来的数据作为Unicode字符串返回.
4. body: 将返回来的数据作为 bytes 字符串返回.
5. xpath: xpath选择器.
6. css: css选择器.

##### 发送POST 请求:
如果想要在请求数据的时候发送post请求,那么这时候需要使用 request 的子类 FormRequest 来实现. 如果想要在爬虫一开始的时候就发送 POST 请求, 那么需要在爬虫类中重写 start_reuqest(self) 方法, 并且不在调用 start_urls 里的url.

