# 集群维护

## 目录

[toc]

**[kubeasz/docs/op at master · easzlab/kubeasz (github.com)](https://github.com/easzlab/kubeasz/tree/master/docs/op)**

## 1、添加与删除 master

```bash
# ./ezctl add-master {cluster_name} {master_ip}
ezctl add-master clement_cluster01  192.168.11.15
```

## 2、添加与删除 node

[管理 node 节点 (kubeasz/github.com)]: https://github.com/easzlab/kubeasz/blob/master/docs/op/op-node.md	"ti"

### 2.1、添加 node

kubeasz 添加：

```bash
# 通过 kubeasz 删除 node 节点
./ezctl del-node {cluster_name} {node_ip}

# 本机示例
./ezctl add-node 192.168.11.20

# 验证：
kubectl get nodes -A
```

### 2.2、删除node

手动删除 node 节点

#### 2.2.1、排除节点

排除节点使其不再参与调度（标记与 master 相符）

```bash
root@k8s-master-1:~$ kubectl get nodes -A
NAME            STATUS                     ROLES    AGE   VERSION
192.168.11.11   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.12   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.19   Ready                      node     30d   v1.22.2
192.168.11.20   Ready                      node     14d   v1.22.2

root@k8s-master-1:~$ kubectl cordon 192.168.11.20
node/192.168.11.20 cordoned

root@k8s-master-1:~$ kubectl get nodes -A
NAME            STATUS                     ROLES    AGE   VERSION
192.168.11.11   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.12   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.19   Ready                      node     30d   v1.22.2
192.168.11.20   Ready,SchedulingDisabled   node     14d   v1.22.2

# 可以看到被 cordon 的节点被禁止调度，与 master 节点状态一致，
# 注意被 cordon 的节点仅不参与调度，其中的 pod 仍可以继续运行。
```

#### 2.2.1.2、恢复节点

取消被 cordon 的节点

```bash
root@k8s-master-1:~$ kubectl uncordon 192.168.11.20
node/192.168.11.20 uncordoned

root@k8s-master-1:~$ kubectl get nodes -A
NAME            STATUS                     ROLES    AGE   VERSION
192.168.11.11   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.12   Ready,SchedulingDisabled   master   30d   v1.22.2
192.168.11.19   Ready                      node     30d   v1.22.2
192.168.11.20   Ready                      node     14d   v1.22.2
```

#### 2.2.2、驱逐 pod

[emptyDir-临时数据卷](https://www.cnblogs.com/scajy/p/15661554.html)

对节点做污点标签

```bash
# --ignore-daemonsets: 忽略daemonsets控制器
# --delete-emptydir-data： 删除临时数据卷
# --ignore-errors： 忽略即将废弃命令的错误
root@k8s-master-1:~$ kubectl drain 192.168.11.20 --ignore-errors --ignore-daemonsets --force
node/192.168.11.20 already cordoned
WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/net-nginx1; ignoring DaemonSet-managed Pods: kube-system/calico-node-nxxk5
evicting pod kube-system/coredns-5dfd59d5d8-db8rh
evicting pod default/net-nginx1
evicting pod kube-system/calico-kube-controllers-59df8b6856-9jdvf
pod/calico-kube-controllers-59df8b6856-9jdvf evicted
pod/net-nginx1 evicted
pod/coredns-5dfd59d5d8-db8rh evicted
node/192.168.11.20 evicted

```

##### 验证

```bash
# 注意 node 节点状态：SchedulingDisabled
root@k8s-master-1:~$ kubectl get node
NAME            STATUS                     ROLES    AGE   VERSION
192.168.11.11   Ready,SchedulingDisabled   master   16d   v1.22.2
192.168.11.12   Ready,SchedulingDisabled   master   16d   v1.22.2
192.168.11.19   Ready                      node     16d   v1.22.2
192.168.11.20   Ready,SchedulingDisabled   node     16d   v1.22.2

# 查看 node 中 运行的 pod （也许有更好的方法）
root@k8s-master-1:~$ kubectl get pods -A -o wide|grep 192.168.11.20
kube-system  calico-node-nxxk5  1/1  Running  3 (13d ago)  16d  192.168.11.20  192.168.11.20  <none>  <none>

# 查看 node 详细信息： 在 containers 中可以看到所有 node 中运行的所有容器信息
root@k8s-master-1:~$ kubectl describe node 192.168.11.20 -n default
Conditions:
Type            Status    LastHeartbeatTime    LastTransitionTime  Reason            Message
----            ------    -----------------    ------------------  ------            -------
DiskPressure    False     Sun, 30..13:43:49    Fri, .. 04:29:11     Kubele..ressure  kubelet has..
PIDPressure     False     Sun, 30..13:43:49    Fri, .. 04:29:11     Kubele..ientPID  kubelet has..
Ready           True      Sun, 30..13:43:49    Fri, .. 04:29:11     KubeletReady     kubelet is ..

```

#### 2.2.2、从 k8s 集群中删除 node

```bash
root@k8s-master-1:~$ kubectl delete node 192.168.11.20
node "192.168.11.20" deleted

# 查看节点
root@k8s-master-1:~$ kubectl get nodes 
NAME            STATUS                     ROLES    AGE   VERSION
192.168.11.11   Ready,SchedulingDisabled   master   16d   v1.22.2
192.168.11.12   Ready,SchedulingDisabled   master   16d   v1.22.2
192.168.11.19   Ready                      node     16d   v1.22.2
```

### 2.3、Kubeasz 删除示例：

```bash
# 通过 kubeasz 删除 node 节点
./ezctl del-node {cluster_name} {node_ip}

# 查看 cluster_name
ls /etc/kubeasz/clusters
>>> clemente-cluster01

# 本机示例
./ezctl del-node clemente-cluster01 192.168.11.20
```

### 2.4、注意事项

删除 node 节点后，需要手动删除 `/{$kubeasz_path}/clusters/{cluster_name}/hosts` 文件中 node 节点的信息！

# 集群升级

[kubeasz/upgrade.md at master · easzlab/kubeasz (github.com)](https://github.com/easzlab/kubeasz/blob/master/docs/op/upgrade.md)

k8s 集群升级注意事项：

1. **不跨大版本升级**
2. **先升级 master ，再升级 node。**
   **（反之可能会因为 node 节点版本过新，导致 master 调度时出错。例如删除旧版本 API 接口、更改文件路径等）**
3. **master 节点升级时，注意核实 node 节点中 kube-lb 中的信息！（有可能不是负载均衡地址）**

**集群升级的本质：在 GitHub 下载新版二进制包，进行替换（停止服务）。**

## 3、升级 master

```bash
# 所需升级的服务
kube-apiserver  kube-controller-manager  kubelet  kube-proxy  kube-scheduler
```

1.下载新版本二进制包，1.23.0 为例：[kubernetes/CHANGELOG-1.23.md/kubernetes (github.com)](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#downloads-for-v1233)

2.在负载均衡中去除需要升级的节点

3.停止服务：`systemctl stop kube-apiserver  kube-controller-manager  kubelet  kube-proxy  kube-scheduler`

4.替换新版：

```bash
scp \
kube-apiserver  kube-controller-manager  kubelet  kube-proxy  kube-scheduler \
root@k8s-master3:/usr/bin/
```

5.启动服务：`systemctl start kube-apiserver  kube-controller-manager  kubelet  kube-proxy  kube-scheduler`

6.验证节点及可用性

7.在负载均衡中加入已经升级的节点

**其余节点以此反复**

### 3.1、注意事项

**升级 kubeasz 分发目录的 版本**

验证升级成功后，应第一时间升级替换 kubeasz 分发目录，即 `/etc/kubeasz/bin`，否则新增节点时依然为旧版本。亦可增加 master 新版本节点，而后做旧版替换。

## 4、升级 node

```bash
# 所需升级的服务
kubelet  kube-proxy
# 如有必要可连同 kubectl 一同升级，升级后需验证。
# 在 master 中使用的 kubectl get nodes 命令，展示的是 kubelet 版本，所以会有可能出现  kube-proxy 版本不一致的情况。
```

升级步骤与 master 类似

## 5、master 高可用机制与验证
